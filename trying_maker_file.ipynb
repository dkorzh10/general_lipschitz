{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985ec373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting certify_gl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile certify_gl.py\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "import yaml\n",
    "\n",
    "import click\n",
    "import scipy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "# from torchvision.models.resnet import resnet50\n",
    "\n",
    "from architectures import get_architecture\n",
    "from datasets_utils import get_dataset, DATASETS, get_num_classes, get_normalize_layer\n",
    "from src.numerical_methods import *\n",
    "from src.certification_utils import *\n",
    "from src.smoothing_and_attacks import *\n",
    "from src.utils import *\n",
    "\n",
    "\n",
    "def make_test_dataset(config):\n",
    "    test_dataset = get_dataset(config[\"dataset\"], \"test\")\n",
    "    pin_memory = (config[\"dataset\"] == \"imagenet\")\n",
    "    np.random.seed(42)\n",
    "    idxes = np.random.choice(len(test_dataset), config[\"NUM_IMAGES_FOR_TEST\"], replace=False)\n",
    "    \n",
    "    ourdataset = make_our_dataset_v2(test_dataset, idxes)\n",
    "    ourdataloader = DataLoader(ourdataset, shuffle=False, batch_size=1,\n",
    "                         num_workers=6, pin_memory=False)\n",
    "    return ourdataset, ourdataloader\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    device = torch.device(config[\"device\"])\n",
    "    model = get_architecture(arch=config[\"arch\"], dataset=config[\"dataset\"], device=device)\n",
    "    checkpoint = torch.load(config[\"base_classifier\"], map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_bounds(ns, b_zero, x0, d, betas_list, type_of_transform, sigmas=[1, 1, 1, 1, 1]):\n",
    "    shape = [b.shape[0] for b in betas_list]\n",
    "    print(\"I'm here\")\n",
    "    shape = tuple(shape)\n",
    "    betas = jnp.asarray(list(map(jnp.array, itertools.product(*betas_list))))\n",
    "    sigma_b, sigma_c, sigma_tr, sigma_gamma, sigma_blur = sigmas\n",
    "    gamma = construct_gamma(sigma_b=sigma_b, sigma_c=sigma_c, sigma_tr=sigma_tr, sigma_gamma=sigma_gamma, sigma_blur=sigma_blur)\n",
    "    bounds, p, g = compute_normed_bounds(compute_bound, x0, gamma, b_zero, betas, key, ns, d, type_of_transform)\n",
    "    x, xi = pxi_to_xi(p)\n",
    "    z = csaps(betas_list, g.reshape(shape))\n",
    "    \n",
    "    hg = []\n",
    "\n",
    "    for beta in tqdm(betas):\n",
    "        hat_g = g_to_hat_g(z, beta, b_zero)\n",
    "        hg.append(hat_g)\n",
    "\n",
    "    hat_g = jnp.asarray(hg)\n",
    "\n",
    "    hatg_int = csaps(betas_list, hat_g.reshape(shape)) #\n",
    "    return xi, hatg_int\n",
    "\n",
    "\n",
    "def calculate_general(config):\n",
    "    \n",
    "    # defining and loading main parameters and fucntions\n",
    "    xi_tss = scipy.stats.norm.ppf\n",
    "    \n",
    "    device = torch.device(config[\"device\"])\n",
    "    sigmas = config[\"sigmas\"]\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    n0 = config[\"n0\"]\n",
    "    maxn = config[\"maxn\"]\n",
    "    adaptive = config[\"adaptive\"]\n",
    "    alpha = config[\"alpha\"]\n",
    "    bs = config[\"batch\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "    \n",
    "    \n",
    "    b_zero = jnp.array([1., 0.])\n",
    "    x0 = jnp.array([1.1, 1.3]) # Whatever\n",
    "    d = 2\n",
    "    \n",
    "    # loading base classifier f\n",
    "    model = load_model(config)\n",
    "    \n",
    "    # creating test dataset to certify on\n",
    "    ourdataset, ourdataloader = make_test_dataset(config)\n",
    "    \n",
    "    # constructing smoothing transfrom phi\n",
    "    phi = construct_phi(config[\"transform\"], device=device, **sigmas)\n",
    "    \n",
    "    \n",
    "    # constructiong xi and hatg\n",
    "    b_zero = jnp.array(config[\"b_zero\"])\n",
    "    x0 = jnp.array(config[\"x0\"])\n",
    "    d = config[\"dimenshion\"]\n",
    "    type_of_transform = config[\"transfrom\"]\n",
    "    \n",
    "    betas_dict = config[\"betas_estimation\"]\n",
    "    betas_list = []\n",
    "    for key in betas_dict:\n",
    "        beta = betas_dict[key]\n",
    "        betas_list.append(jnp.linspace(beta[\"left\"], beta[\"right\"], beta[\"db\"]))\n",
    "        \n",
    "    \n",
    "    xi, hatg_int = construct_bounds(ns, b_zero, x0, d, betas_list, type_of_transform)\n",
    "    \n",
    "    # defining and loading attack\n",
    "    attack = construct_attack(type_of_transform)\n",
    "    \n",
    "    # calculating benign (vanilla) accuracy of base classifier f\n",
    "    # Accuracy of non-smoothed model on original images\n",
    "    benign_acc = Accuracy(model, loader=ourdataloader, device=device)\n",
    "    print(f\"Benign accuracy {benign_acc}\")\n",
    "    \n",
    "    # creating attack set B to certify model on\n",
    "    betas_attack_dict = config[\"betas_certification\"]\n",
    "    betas_attack_list = []\n",
    "    for key in betas_attack_dict:\n",
    "        beta_attack = betas_attack_dict[key]\n",
    "        betas_atttack_list.append(np.linspace(beta_attack[\"left\"], beta_attack[\"right\"], beta_attack[\"db\"]))\n",
    "    betas_attack = np.asarray(list(map(np.array, itertools.product(*betas_attack_list))))\n",
    "\n",
    "    # calculating statistics\n",
    "    paCP, isOkCP = pa_isOk_collector(model, loader=ourdataloader, Phi=Phi, device=device,\n",
    "                           n0=n0, maxn=maxn, alpha=alpha, batch_size=bs, adaptive=adaptive,\n",
    "                             num_classes=num_classes)\n",
    "    h_acc = np.mean(isOkCP)\n",
    "    print(f\"Ordinary accuracy of Smoothed Classiifer {h_acc}\")\n",
    "    \n",
    "    \n",
    "    # calculate empirically robust accuracy of f\n",
    "    f_era = None\n",
    "    if config[\"calculate_f_era\"]:\n",
    "        f_era = ERA_Only_ND(model, ourdataloader, attack=attack, device=device, PSN=betas_attack)\n",
    "        print(f\"f_era {f_era}\")\n",
    "    \n",
    "\n",
    "    # calculate empirically robust accuracy of h (might be very time consuming)\n",
    "    h_era = None\n",
    "    if config[\"calculate_f_era\"]:\n",
    "        h_era = ERA_Only_For_Smoothed_ND(model, ourdataloader, attack, Phi, device, \n",
    "                                 PSN=betas_attack, n0=n0, maxn=maxn, alpha=alpha, \n",
    "                                 batch_size=bs, adaptive=adaptive, num_classes=num_classes)\n",
    "        print(f\"h_era {h_era}\")\n",
    "        \n",
    "\n",
    "    # calulate our certified robust accuracy (CRA)\n",
    "    hlist = np.linspace(config[\"hlist\"][\"left\"], config[\"hlist\"][\"right\"], config[\"hlist\"][\"n_steps\"])\n",
    "    hmin_ours = CertAccChecker(safe_beta, betas=betas_attack, hlist=hlist, xi=xi, hatg_int=hatg_int)\n",
    "\n",
    "    if hmin_ours:\n",
    "        cert_acc_ours = ((paCP > hmin_ours).astype(\"int\") * isOkCP).mean()\n",
    "    else:\n",
    "        cert_acc_ours = 0\n",
    "        hmin_ours = None\n",
    "    print(f\"Cert Acc {type_of_transform} ours is {cert_acc_ours}.  h_min is {hmin_ours}\")\n",
    "    \n",
    "\n",
    "    # calculate TSS' CRA if applicable\n",
    "    sb_tss = safe_beta_tss(type_of_transform, **sigmas)\n",
    "    hmin_tss = CertAccCheckerTSS(betas=betas_attack, hlist=hlist, xi=xi_tss, safe_beta_tss=sb_tss)\n",
    "    if hmin_tss:\n",
    "        cert_acc_tss = ((paCP > hmin_tss * isOkCP).mean()\n",
    "    else:\n",
    "        cert_acc_tss = 0\n",
    "        hmin_tss = None\n",
    "    print(f\"Cert Acc {type_of_transform} ours is {cert_acc_tss}.  h_min is {hmin_tss}\")\n",
    "    \n",
    "    \n",
    "    # calculate MP's CRA if applicable\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "@click.command()\n",
    "@click.argument(\"config_path\", type=click.Path(exists=True))\n",
    "def main(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config[\"b_zero\"])\n",
    "    \n",
    "#     if config[\"gpu\"]:\n",
    "#         os.environ['CUDA_VISIBLE_DEVICES']=str(config[\"gpu\"])\n",
    "#         os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "        \n",
    "\n",
    "\n",
    "    calculate_general(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce14bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_path = \"configs/cb/cifar10.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3041cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0\n",
      "beta1\n"
     ]
    }
   ],
   "source": [
    "for beta in config[\"betas_estimation\"]:\n",
    "    print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8839604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta0': {'left': 0.5, 'right': 1.5, 'db': 41},\n",
       " 'beta1': {'left': -0.5, 'right': 0.5, 'db': 43}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"betas_estimation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fcedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    args.corrupt = config[\"attack\"]\n",
    "    args.noise_sd = config[\"noise_sd\"]\n",
    "    args.noise_dst = config[\"noise_dst\"]\n",
    "    args.partial_max = config[\"partial_max\"]\n",
    "    args.dataset = config[\"dataset\"]\n",
    "    args.base_classifier = config[\"base_classifier\"]\n",
    "    args.device = config[\"device\"]\n",
    "    args.arch = config[\"arch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = model\n",
    "    \n",
    "    dataset, _ = make_test_dataset(args)\n",
    "    \n",
    "    base_classifier.to(device)\n",
    "    base_classifier.eval()\n",
    "\n",
    "    corruptor = Corruption(args, co_type=args.corrupt,add_noise=args.add_noise,noise_sd=args.noise_sd,distribution=args.noise_dst)\n",
    "\n",
    "    # create the smooothed classifier g\n",
    "    smoothed_classifier = TSmooth(base_classifier, None, corruptor, get_num_classes(args.dataset),args.noise_dst,args.noise_sd, args.add_noise)\n",
    "\n",
    "    # prepare output file\n",
    "    filename = args.outfile+'_'+args.dataset+'_'+args.corrupt+'_'+str(args.noise_sd) +\"_\" +str(args.partial_max)\n",
    "    f = open(filename+'_running', 'w')\n",
    "    print(\"idx\\tlabel\\tpredict\\tradius\\tgood\\tcorrect\\ttime\", file=f, flush=True)\n",
    "    print(\"idx\\tlabel\\tpredict\\tradius\\tgood\\tcorrect\\ttime\")\n",
    "\n",
    "    tot, tot_good, tot_correct = 0, 0, 0\n",
    "\n",
    "    # for gaussian smooth\n",
    "    attack_radius = args.partial_max\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        (x, label) = dataset[i]\n",
    "\n",
    "        before_time = time()\n",
    "        x = x.to(device)\n",
    "        prediction, radius = smoothed_classifier.certify(x, args.N0, args.N, args.alpha, args.batch)\n",
    "\n",
    "\n",
    "        correct = (prediction == label).item()\n",
    "        cond1 = radius * args.noise_sd > args.partial_max\n",
    "        good = (radius * args.noise_sd > args.partial_max)&correct\n",
    "\n",
    "        tot, tot_good, tot_correct = tot+1, tot_good+good, tot_correct+correct\n",
    "        after_time = time()\n",
    "        time_elapsed = str(datetime.timedelta(seconds=(after_time - before_time)))\n",
    "        print(\"{}\\t{}\\t{}\\t{:.5f}\\t{}\\t{}\\t{}\".format(i, label, prediction, radius, good, correct, time_elapsed), file=f, flush=True)\n",
    "        print(\"{}\\t{}\\t{}\\t{:.5f}\\t{}\\t{}\\t{}\".format(i, label, prediction, radius, good, correct, time_elapsed))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print(\"Total {} Certified {} Certified Acc {} Test Acc {}\".format(tot, tot_good, tot_good/tot, tot_correct/tot))\n",
    "\n",
    "    f = open(filename+'_total_result', 'w')\n",
    "    print(\"Total {} Certified {} Certified Acc {} Test Acc {}\".format(tot, tot_good, tot_good/tot, tot_correct/tot), file=f, flush=True)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
