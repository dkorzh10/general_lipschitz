{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab1b0c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  1 21:53:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              60W / 300W |  12447MiB / 16384MiB |      1%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              61W / 300W |   1531MiB / 16384MiB |      0%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              42W / 300W |      3MiB / 16384MiB |      0%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              60W / 300W |  12447MiB / 16384MiB |      1%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-16GB           On  | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   53C    P0             181W / 300W |  14909MiB / 16384MiB |     65%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-16GB           On  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   62C    P0             266W / 300W |  13321MiB / 16384MiB |     18%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-16GB           On  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   55C    P0             261W / 300W |  15319MiB / 16384MiB |    100%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-16GB           On  | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              61W / 300W |  12447MiB / 16384MiB |      1%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54e3c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting certify_gl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile certify_gl.py\n",
    "from datetime import datetime, date\n",
    "import sys\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"2\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import click\n",
    "import scipy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "\n",
    "from architectures import get_architecture\n",
    "from datasets_utils import get_dataset, DATASETS, get_num_classes, get_normalize_layer\n",
    "from src.numerical_methods import *\n",
    "from src.certification_utils import *\n",
    "from src.smoothing_and_attacks import *\n",
    "from src.utils import *\n",
    "\n",
    "\n",
    "def make_test_dataset(config):\n",
    "    test_dataset = get_dataset(config[\"dataset\"], \"test\")\n",
    "    pin_memory = (config[\"dataset\"] == \"imagenet\")\n",
    "    np.random.seed(42)\n",
    "    idxes = np.random.choice(len(test_dataset), config[\"NUM_IMAGES_FOR_TEST\"], replace=False)\n",
    "    \n",
    "    ourdataset = make_our_dataset_v2(test_dataset, idxes)\n",
    "    ourdataloader = DataLoader(ourdataset, shuffle=False, batch_size=1,\n",
    "                         num_workers=6, pin_memory=False)\n",
    "    return ourdataset, ourdataloader\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    device = torch.device(config[\"device\"])\n",
    "    model = get_architecture(arch=config[\"arch\"], dataset=config[\"dataset\"], device=device)\n",
    "    checkpoint = torch.load(config[\"base_classifier\"], map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_bounds(ns, b_zero, x0, d, betas_list, type_of_transform, sigmas):\n",
    "    shape = [b.shape[0] for b in betas_list]\n",
    "    print(\"I'm here\")\n",
    "    shape = tuple(shape)\n",
    "    betas = jnp.asarray(list(map(jnp.array, itertools.product(*betas_list))))\n",
    "    sigma_b, sigma_c, sigma_tr, sigma_gamma, sigma_blur = sigmas\n",
    "    gamma = construct_gamma(sigma_b=sigma_b, sigma_c=sigma_c, sigma_tr=sigma_tr, sigma_gamma=sigma_gamma, sigma_blur=sigma_blur)\n",
    "    bounds, p, g = compute_normed_bounds(compute_bound, x0, gamma, b_zero, betas, key, ns, d, type_of_transform)\n",
    "    x, xi = pxi_to_xi(p)\n",
    "    z = csaps(betas_list, g.reshape(shape))\n",
    "    \n",
    "    hg = []\n",
    "\n",
    "    for beta in tqdm(betas):\n",
    "        hat_g = g_to_hat_g(z, beta, b_zero)\n",
    "        hg.append(hat_g)\n",
    "\n",
    "    hat_g = jnp.asarray(hg)\n",
    "\n",
    "    hatg_int = csaps(betas_list, hat_g.reshape(shape)) #\n",
    "    return xi, hatg_int\n",
    "\n",
    "def do_log(filename, string):\n",
    "    with open(filename, \"a\") as f:\n",
    "        print(string, file=f, flush=True)\n",
    "        \n",
    "\n",
    "def calculate_general(config):\n",
    "    \n",
    "    # defining and loading main parameters and fucntions\n",
    "    xi_tss = scipy.stats.norm.ppf\n",
    "    \n",
    "    device = torch.device(config[\"device\"])\n",
    "    sigmas = config[\"sigmas\"]\n",
    "\n",
    "    n0 = config[\"n0\"]\n",
    "    maxn = config[\"maxn\"]\n",
    "    adaptive = config[\"adaptive\"]\n",
    "    alpha = config[\"alpha\"]\n",
    "    bs = config[\"batch\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "    \n",
    "    \n",
    "    b_zero = jnp.array(config[\"b_zero\"])\n",
    "    x0 = jnp.array(config[\"x0\"])\n",
    "    d = config[\"dimenshion\"]\n",
    "    type_of_transform = config[\"transform\"]\n",
    "    ns = config[\"ns\"]\n",
    "    \n",
    "    # creating logfile and saving used config\n",
    "    exp_start_time = \"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.now())\n",
    "    sigmas_values = \"b_{}_c_{}_tr_{}_gamma_{}_blur_{}\".format(*sigmas.values())\n",
    "    exp_dir = os.path.join(config[\"log_dir\"], type_of_transform, config[\"dataset\"], sigmas_values, exp_start_time)\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "    filename = os.path.join(exp_dir, \"res.txt\")\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"used_config.yaml\"), \"w\") as f:\n",
    "        yaml.dump(config, f)\n",
    "        \n",
    "\n",
    "    # loading base classifier f\n",
    "    model = load_model(config)\n",
    "    \n",
    "    # creating test dataset to certify on\n",
    "    ourdataset, ourdataloader = make_test_dataset(config)\n",
    "    \n",
    "    # constructing smoothing transfrom phi\n",
    "    phi = construct_phi(config[\"transform\"], device=device, **sigmas)\n",
    "    \n",
    "    \n",
    "    # constructiong xi and hatg\n",
    "    \n",
    "    betas_dict = config[\"betas_estimation\"]\n",
    "    betas_list = []\n",
    "    for i, key in enumerate(betas_dict):\n",
    "        if i >= d:\n",
    "            break\n",
    "        beta = betas_dict[key]\n",
    "        betas_list.append(jnp.linspace(beta[\"left\"], beta[\"right\"], beta[\"db\"]))\n",
    "        \n",
    "    \n",
    "    xi, hatg_int = construct_bounds(ns, b_zero, x0, d, betas_list, type_of_transform, sigmas.values())\n",
    "    \n",
    "    # defining and loading attack\n",
    "    attack = construct_attack(type_of_transform)\n",
    "    \n",
    "    # calculating benign (vanilla) accuracy of base classifier f\n",
    "    # accuracy of non-smoothed model on original images\n",
    "    benign_acc = Accuracy(model, loader=ourdataloader, device=device)\n",
    "    benign_acc = benign_acc.mean()\n",
    "    string = f\"Benign accuracy {benign_acc}\"\n",
    "    print(string)\n",
    "    do_log(filename, string)\n",
    "    \n",
    "    \n",
    "    # creating attack set B to certify model on\n",
    "    betas_attack_dict = config[\"betas_certification\"]\n",
    "    betas_attack_list = []\n",
    "    for i, key in enumerate(betas_attack_dict):\n",
    "        if i >= d:\n",
    "            break\n",
    "        beta_attack = betas_attack_dict[key]\n",
    "        betas_attack_list.append(np.linspace(beta_attack[\"left\"], beta_attack[\"right\"], beta_attack[\"db\"]))\n",
    "    betas_attack = np.asarray(list(map(np.array, itertools.product(*betas_attack_list))))\n",
    "\n",
    "    # calculating statistics\n",
    "    paCP, isOkCP = pa_isOk_collector(model, loader=ourdataloader, Phi=phi, device=device,\n",
    "                           n0=n0, maxn=maxn, alpha=alpha, batch_size=bs, adaptive=adaptive,\n",
    "                             num_classes=num_classes)\n",
    "    res_dict = {}\n",
    "    res_dict[\"pa\"] = list(paCP)\n",
    "    res_dict[\"is_ok\"] = list(map(int, list(isOkCP)))\n",
    "    with open(os.path.join(exp_dir, \"res_dict.json\"), 'w') as f:\n",
    "        json.dump(res_dict, f, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "    h_acc = np.mean(isOkCP)\n",
    "    string = f\"Ordinary accuracy of Smoothed Classiifer {h_acc}\"\n",
    "    print(string)\n",
    "    do_log(filename, string)\n",
    "    \n",
    "    # calculate empirically robust accuracy of f\n",
    "    f_era = None\n",
    "    if config[\"calculate_f_era\"]:\n",
    "        f_era = ERA_Only_ND(model, ourdataloader, attack=attack, device=device, PSN=betas_attack)\n",
    "        print(f\"f_era {f_era}\")\n",
    "        do_log(filename, string)\n",
    "\n",
    "    # calculate empirically robust accuracy of h (might be very time consuming)\n",
    "    h_era = None\n",
    "    if config[\"calculate_f_era\"]:\n",
    "        h_era = ERA_Only_For_Smoothed_ND(model, ourdataloader, attack, phi, device, \n",
    "                                 PSN=betas_attack, n0=n0, maxn=maxn, alpha=alpha, \n",
    "                                 batch_size=bs, adaptive=adaptive, num_classes=num_classes)\n",
    "        print(f\"h_era {h_era}\")\n",
    "        do_log(filename, string)\n",
    "\n",
    "    # calulate our certified robust accuracy (CRA)\n",
    "    hlist = np.linspace(config[\"hlist\"][\"left\"], config[\"hlist\"][\"right\"], config[\"hlist\"][\"n_steps\"])\n",
    "    print(hlist)\n",
    "    hmin_ours = CertAccChecker(safe_beta, betas=betas_attack, hlist=hlist, xi=xi, hatg_int=hatg_int)\n",
    "    print(\"HMIN!!!!!\", hmin_ours)\n",
    "    if hmin_ours:\n",
    "        cert_acc_ours = ((paCP > hmin_ours).astype(\"int\") * isOkCP).mean()\n",
    "    else:\n",
    "        cert_acc_ours = 0\n",
    "        hmin_ours = None\n",
    "    string = f\"Cert Acc {type_of_transform} ours is {cert_acc_ours}.  h_min is {hmin_ours}\"\n",
    "    print(string)\n",
    "    do_log(filename, string)\n",
    "\n",
    "\n",
    "    # calculate TSS' CRA if applicable\n",
    "    sb_tss = safe_beta_tss(type_of_transform, **sigmas)\n",
    "    cert_acc_tss = None\n",
    "    hmin_tss = None\n",
    "    if sb_tss:\n",
    "        hmin_tss = CertAccCheckerTSS(betas=betas_attack, hlist=hlist, xi=xi_tss, safe_beta_tss=sb_tss)\n",
    "        if hmin_tss:\n",
    "            cert_acc_tss = ((paCP > hmin_tss).astype(\"int\") * isOkCP).mean()\n",
    "        else:\n",
    "            cert_acc_tss = 0\n",
    "            hmin_tss = None\n",
    "    string = f\"Cert Acc {type_of_transform} TSS is {cert_acc_tss}.  h_min is {hmin_tss}\"\n",
    "    print(string)\n",
    "    do_log(filename, string)\n",
    "\n",
    "\n",
    "    # calculate MP's CRA if applicable\n",
    "    cert_acc_mp = None\n",
    "    hmin_mp = None\n",
    "    if type_of_transform in [\"c\", \"gamma\"]:\n",
    "        hmin_mp = CertAccCheckerTSS(betas=betas_attack, hlist=hlist, xi=None, safe_beta_tss=safe_beta_MP_gamma)\n",
    "        if hmin_mp:\n",
    "            cert_acc_mp = ((paCP > hmin_mp).astype(\"int\") * isOkCP).mean()\n",
    "        else:\n",
    "            cert_acc_mp = 0\n",
    "            hmin_mp = None\n",
    "    string = f\"Cert Acc {type_of_transform} MP is {cert_acc_mp}.  h_min is {hmin_mp}\"\n",
    "    print(string)\n",
    "    do_log(filename, string)\n",
    "    \n",
    "\n",
    "    \n",
    "@click.command()\n",
    "@click.argument(\"config_path\", type=click.Path(exists=True))\n",
    "def main(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    calculate_general(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24b5dbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 16.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linspace(1, 16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59bb8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849/1519491582.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  r = lambda h: -np.log(2 - 2 * h)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.02020271, 0.03025304, 0.04040541, 0.05066191, 0.0610247 ,\n",
       "       0.071496  , 0.08207811, 0.0927734 , 0.10358432, 0.11451339,\n",
       "       0.12556322, 0.13673652, 0.14803608, 0.15946477, 0.1710256 ,\n",
       "       0.18272164, 0.19455609, 0.20653229, 0.21865365, 0.23092374,\n",
       "       0.24334626, 0.25592504, 0.26866407, 0.28156747, 0.29463955,\n",
       "       0.30788478, 0.3213078 , 0.33491345, 0.34870677, 0.36269302,\n",
       "       0.37687765, 0.39126639, 0.40586519, 0.42068027, 0.43571815,\n",
       "       0.45098562, 0.46648981, 0.48223817, 0.49823851, 0.51449903,\n",
       "       0.53102833, 0.54783545, 0.56492988, 0.58232163, 0.6000212 ,\n",
       "       0.61803971, 0.63638885, 0.65508098, 0.67412917, 0.69354726,\n",
       "       0.71334989, 0.7335526 , 0.75417188, 0.77522529, 0.7967315 ,\n",
       "       0.8187104 , 0.84118326, 0.86417278, 0.88770328, 0.91180083,\n",
       "       0.93649344, 0.96181125, 0.98778673, 1.01445498, 1.04185395,\n",
       "       1.07002483, 1.09901237, 1.12886533, 1.15963699, 1.19138569,\n",
       "       1.22417551, 1.25807706, 1.29316838, 1.32953603, 1.36727636,\n",
       "       1.40649707, 1.44731906, 1.48987868, 1.53433044, 1.58085046,\n",
       "       1.62964062, 1.68093391, 1.73500114, 1.79215955, 1.85278417,\n",
       "       1.91732269, 1.98631556, 2.06042354, 2.14046624, 2.22747762,\n",
       "       2.3227878 , 2.42814832, 2.54593135, 2.67946274, 2.83361342,\n",
       "       3.01593498, 3.23907853, 3.5267606 , 3.93222571, 4.62537289,\n",
       "              inf])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = lambda h: -np.log(2 - 2 * h)\n",
    "h = np.linspace(0.51, 1, 101)\n",
    "\n",
    "r(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d032c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_path = \"configs/cb/cifar10.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "917f3149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta0\n",
      "beta1\n"
     ]
    }
   ],
   "source": [
    "for beta in config[\"betas_estimation\"]:\n",
    "    print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "531b74f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta0': {'left': 0.5, 'right': 1.5, 'db': 41},\n",
       " 'beta1': {'left': -0.5, 'right': 0.5, 'db': 43}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"betas_estimation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4774e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    args.corrupt = config[\"attack\"]\n",
    "    args.noise_sd = config[\"noise_sd\"]\n",
    "    args.noise_dst = config[\"noise_dst\"]\n",
    "    args.partial_max = config[\"partial_max\"]\n",
    "    args.dataset = config[\"dataset\"]\n",
    "    args.base_classifier = config[\"base_classifier\"]\n",
    "    args.device = config[\"device\"]\n",
    "    args.arch = config[\"arch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = model\n",
    "    \n",
    "    dataset, _ = make_test_dataset(args)\n",
    "    \n",
    "    base_classifier.to(device)\n",
    "    base_classifier.eval()\n",
    "\n",
    "    corruptor = Corruption(args, co_type=args.corrupt,add_noise=args.add_noise,noise_sd=args.noise_sd,distribution=args.noise_dst)\n",
    "\n",
    "    # create the smooothed classifier g\n",
    "    smoothed_classifier = TSmooth(base_classifier, None, corruptor, get_num_classes(args.dataset),args.noise_dst,args.noise_sd, args.add_noise)\n",
    "\n",
    "    # prepare output file\n",
    "    filename = args.outfile+'_'+args.dataset+'_'+args.corrupt+'_'+str(args.noise_sd) +\"_\" +str(args.partial_max)\n",
    "    f = open(filename+'_running', 'w')\n",
    "    print(\"idx\\tlabel\\tpredict\\tradius\\tgood\\tcorrect\\ttime\", file=f, flush=True)\n",
    "    print(\"idx\\tlabel\\tpredict\\tradius\\tgood\\tcorrect\\ttime\")\n",
    "\n",
    "    tot, tot_good, tot_correct = 0, 0, 0\n",
    "\n",
    "    # for gaussian smooth\n",
    "    attack_radius = args.partial_max\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        (x, label) = dataset[i]\n",
    "\n",
    "        before_time = time()\n",
    "        x = x.to(device)\n",
    "        prediction, radius = smoothed_classifier.certify(x, args.N0, args.N, args.alpha, args.batch)\n",
    "\n",
    "\n",
    "        correct = (prediction == label).item()\n",
    "        cond1 = radius * args.noise_sd > args.partial_max\n",
    "        good = (radius * args.noise_sd > args.partial_max)&correct\n",
    "\n",
    "        tot, tot_good, tot_correct = tot+1, tot_good+good, tot_correct+correct\n",
    "        after_time = time()\n",
    "        time_elapsed = str(datetime.timedelta(seconds=(after_time - before_time)))\n",
    "        print(\"{}\\t{}\\t{}\\t{:.5f}\\t{}\\t{}\\t{}\".format(i, label, prediction, radius, good, correct, time_elapsed), file=f, flush=True)\n",
    "        print(\"{}\\t{}\\t{}\\t{:.5f}\\t{}\\t{}\\t{}\".format(i, label, prediction, radius, good, correct, time_elapsed))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print(\"Total {} Certified {} Certified Acc {} Test Acc {}\".format(tot, tot_good, tot_good/tot, tot_correct/tot))\n",
    "\n",
    "    f = open(filename+'_total_result', 'w')\n",
    "    print(\"Total {} Certified {} Certified Acc {} Test Acc {}\".format(tot, tot_good, tot_good/tot, tot_correct/tot), file=f, flush=True)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22ff4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d60bb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(0, 0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb47d84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024_02_01_20_07_59'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "defc66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024_02_01_Feb_02\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "print(\"{:%Y_%m_%d_%h_%m}\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd9eb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 2, 1, 19, 36, 59, 701883)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49409357",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [1, 2, 3, 4, 5.5]\n",
    "\n",
    "sigmas_values = \"b_{}_c_{}_tr_{}_gamma_{}_blur_{}\".format(*sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a82abac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b_1_c_2_tr_3_gamma_4_blur_5.5'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "308b38af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024_02_01_20_35_05'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "str(datetime.now())\n",
    "\"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d55014b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.3, 0.3, None, None, None])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"sigmas\"].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "276acae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas_values = \"b_{}_c_{}_tr_{}_gamma_{}_blur_{}\".format(*config[\"sigmas\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fd4a98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b_0.3_c_0.3_tr_None_gamma_None_blur_None'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4759a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
